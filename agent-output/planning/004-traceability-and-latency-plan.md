# Plan 004: End-to-End Traceability & Latency

**Plan ID**: 004
**Target Release**: v0.2.1 (Performance Validation)
**Epic**: 1.4 End-to-End Traceability & Latency (Shared / Integration)
**Status**: Proposed
**Date**: 2026-01-19
**Dependencies**: Plan 001 (Infra), Plan 002 (ASR), Plan 003 (Translation)

## 1. Value Statement and Business Objective
**As a** Researcher/Thesis Author,
**I want** to trace a single request from Audio In to Translated Text Out and measure the time,
**So that** I can validate the performance claims of the architecture.

**Measure of Success**:
- A script/tool exists that can run 100 sequential requests.
- Latency is calculated as `Timestamp(TextTranslatedEvent) - Timestamp(AudioInputEvent)`.
- Intermediate latency (ASR completion) is also visible.
- Correlation chain is proven unbroken for >95% of requests.

## 2. Context & Roadmap Alignment
This epic validates the "Walking Skeleton" (v0.2.0) by adding the observability layer required for the thesis. It does not introduce new functional services but adds the "Thesis Probe" machinery.

**Roadmap Reference**:
- **Release v0.2.1**: Performance Validation (Epic 1.4)
- **Priority**: P0

**Architecture Guidance**:
- **Non-Invasive**: The probe acts as an external Kafka client (Producer + Consumer). IT DOES NOT require code changes to ASR or Translation services, assuming they correctly propagate `correlation_id` and set `timestamp` (which is enforced by the schema).
- **Metric Definitions**:
    - `T0`: Audio Ingress Timestamp
    - `T1`: ASR Output Timestamp
    - `T2`: Translation Output Timestamp
    - `Latency_ASR` = T1 - T0
    - `Latency_Translation` = T2 - T1
    - `Latency_Total` = T2 - T0

## 3. Assumptions & Constraints
- **assumption**: ASR and Translation services behave correctly regarding `correlation_id` propagation (verified in v0.2.0).
- **assumption**: Kafka timestamps or Event payload `timestamp` fields are reliable. We will prefer **Event Payload Timestamps** as they represent application-layer intent.
- **constraint**: Must use `speech-lib` for serialization/deserialization.
- **constraint**: The test mechanism must be runnable from the local environment (outside Docker) or a dedicated test container. (Local preferred for ease of MVP).

## 4. Implementation Plan

### Milestone 1: Traceability Infrastructure
**Objective**: Create the probe tool structure.
1.  **Directory**: Create `tests/e2e/`.
2.  **Dependencies**: Ensure `tests/requirements.txt` or dev dependencies include `speech-lib`, `confluent-kafka` (or `kafka-python` if currently used), `numpy` (for stats).

### Milestone 2: The Traceability Probe (Script)
**Objective**: Implement the `measure_latency.py` tool.
1.  **Producer**: Generates a synthetic `AudioInputEvent` (using `synthetic-wav-sample` skill or pre-generated file).
2.  **Consumer**: Listens to **both** `speech.asr.text` and `speech.translation.text`.
3.  **Correlation Logic**:
    - Maintains a `Dict[correlation_id, StartTime]`.
    - On `TextRecognizedEvent`: Record `T1`.
    - On `TextTranslatedEvent`: Record `T2`, calculate Total Latency, print/log, and mark complete.
4.  **Timeout**: Implement a timeout (e.g., 10s) to detect dropped chains.

### Milestone 3: Validation Protocol & Report
**Objective**: Run the "N=100" campaign.
1.  **Loop**: Script accepts `--count 100`.
2.  **Stats**: At the end of the run, print P50, P90, P99 latency.
3.  **CI/Verification**: Define success criteria (e.g., "Mean Latency < 5s" for MVP, though currently we just want *measurement*, not strict SLOs yet).

### Milestone 4: Documentation & Versioning
**Objective**: Release v0.2.1.
1.  **Docs**: Create `docs/benchmarks/001-mvp-latency-baseline.md` to store the first official run results.
2.  **Version**: Bump repository state to v0.2.1 (if applicable) or tag the repo.

## 5. Verification Strategy (QA Handoff)
- **Unit Tests**: None for the script itself (it *is* a test).
- **Operational Verification**:
    1.  Bring up the stack (`ASR`, `Translation`, `Kafka`, `SR`).
    2.  Run `python tests/e2e/measure_latency.py --count 5`.
    3.  Verify output shows `T0`, `T1`, `T2` and deltas.
    4.  Verify no crashes.
- **Data Validation**:
    - Check that calculated latency is positive (T2 > T1 > T0).
    - Synchronize clocks if running across machines (not an issue for local MVP).

## 6. Risks
- **Clock Skew**: If services run in containers with drifted clocks vs the host script.
    - *Mitigation*: We rely on the *event* timestamps generated by the services. If the container clocks are off, the metrics are off. Docker usually syncs with host, but we should verify.
- **Cold Starts**: The first few requests might be slow (model loading). The script should ideally support a "warmup" phase or we accept the P99 spike.

