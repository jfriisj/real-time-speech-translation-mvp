# Implementation 003: Text-to-Text Translation (Translation Service)

## Plan Reference
- Plan: `agent-output/planning/003-translation-service-plan.md`
- Architecture master: `agent-output/architecture/system-architecture.md`

## Date
2026-01-15

## Changelog
| Date | Handoff / Trigger | Summary |
|------|-------------------|---------|
| 2026-01-15 | Plan 003 approved | Implemented Translation Service consuming `speech.asr.text` and producing `speech.translation.text`, including Docker/Compose wiring, unit tests, and opt-in integration test. |
| 2026-01-15 | No-mocks requirement | Replaced mock translation with a real Hugging Face model-backed translator (`Helsinki-NLP/opus-mt-en-es` by default), refactored tests to avoid translator mocks, and added model configuration env vars. |
| 2026-01-15 | Quality gate | Ran Ruff lint + Vulture scan for changed Python code; PASS (0 issues, 0 dead-code findings). |
| 2026-01-15 | Repo hygiene | Removed tracked `__pycache__` binaries and untracked `.vscode/mcp.json` to prevent secret scanning issues. |
| 2026-01-16 | QA gap fixes | Added unit coverage for `process_event` and consumer commit/poll helpers; updated translation Dockerfile to install CPU-only PyTorch for reliable Compose builds. |

## Implementation Summary
Implemented a new microservice under `services/translation/` that:
- Consumes `TextRecognizedEvent` from Kafka topic `speech.asr.text`
- Produces `TextTranslatedEvent` to Kafka topic `speech.translation.text`
- Preserves `correlation_id` end-to-end
- Translates using a real Hugging Face seq2seq model (default: `Helsinki-NLP/opus-mt-en-es`) with in-process model caching

This delivers the Plan 003 value statement by providing a working Audio -> Text -> Translated Text pipeline step (text-to-text), aligned to the canonical topics and Avro schemas mandated by the architecture master.

Note: Plan 003 originally described a mock-first approach to de-risk ML dependencies during the walking-skeleton phase. This implementation intentionally deviates due to the updated requirement: “no mocks allowed”.

## Milestones Completed
- [x] Milestone 1: Service initialization (package + Docker)
- [x] Milestone 2: Translation core (Hugging Face translator + unit tests)
- [x] Milestone 3: Kafka consumer/producer loop with schema registry integration
- [x] Milestone 4: Docker Compose wiring
- [x] Milestone 5: Version management (`0.2.0` + service changelog)

## Files Modified
| Path | Change |
|------|--------|
| `docker-compose.yml` | Added `translation-service` container.
| `shared/speech-lib/src/speech_lib/consumer.py` | Added optional `config` to Confluent consumer + `poll_with_message()` + `commit_message()` to support commit-on-drop semantics.
| `.gitignore` | Ensure `.env` and `.vscode/` are ignored to prevent secret leakage.
| `pytest.ini` | Registered `integration` marker at repo root (removes pytest warnings).
| `services/translation/Dockerfile` | Install CPU-only PyTorch and avoid CUDA dependencies in Docker builds.
| `services/translation/README.md` | Document CPU-only Docker install note.
| `services/translation/pyproject.toml` | Bumped torch minimum version to 2.6.0.
| `services/translation/tests/test_integration_translation.py` | Improve schema dir resolution and extend integration timeout.

## Files Created
| Path | Purpose |
|------|---------|
| `services/translation/pyproject.toml` | Python package definition for the service.
| `services/translation/Dockerfile` | Container build for the service.
| `services/translation/README.md` | Service-level run/config documentation.
| `services/translation/CHANGELOG.md` | Service changelog for v0.2.0.
| `services/translation/src/translation_service/*` | Translation service runtime code (config + main loop + Hugging Face translator).
| `services/translation/tests/*` | Unit tests + opt-in integration test.
| `shared/speech-lib/tests/test_consumer.py` | Unit tests for consumer wrapper commit/poll helpers.

## Code Quality Validation
### Ruff (lint)
- Tool: `analyzer/ruff-check`
- Scope: changed/new Python code in `shared/speech-lib` and `services/translation`
- Result: PASS (0 issues)

### Dead-code scan (Vulture)
- Tool: `analyzer/vulture-scan`
- Scope: translation + consumer wrapper core modules
- Result: PASS

## Value Statement Validation
**Original value statement** (Plan 003):
- As a User, I want recognized text to be translated into my target language, so that I can understand the content.

**Implementation delivers**:
- Service consumes recognized text events (`speech.asr.text`) and emits translated text events (`speech.translation.text`) with preserved `correlation_id`.
- Translated text is generated by a real translation model; output is non-empty and includes schema-required language metadata.

## Behavioral Decisions (Plan Alignment)
- `TARGET_LANGUAGE` environment variable controls `payload.target_language` (default `es`).
- `TRANSLATION_MODEL_NAME` configures the Hugging Face model id (default `Helsinki-NLP/opus-mt-en-es`).
- `MAX_NEW_TOKENS` controls generation length (default `256`).
- Missing/blank `payload.language` from input defaults to `en`.
- For the default MarianMT model, the service validates the requested language pair as `en`→`es` (fails closed rather than silently producing incorrect output).
- "Log and drop" is implemented as: log the error and commit the Kafka offset to prevent poison-pill loops.

## Test Coverage
### Unit
- `services/translation/tests/test_translator.py`
- `services/translation/tests/test_translation_processing.py`
- `shared/speech-lib/tests/test_consumer.py`

### Integration (opt-in)
- `services/translation/tests/test_integration_translation.py`
- Opt-in guard: `RUN_TRANSLATION_INTEGRATION=1`

## Test Execution Results
### Full suite
- Command: `python -m pytest -q`
- Result: `12 passed`

### Targeted tests (QA gap coverage)
- Command: `python -m pytest shared/speech-lib/tests/test_consumer.py services/translation/tests/test_translation_processing.py -q`
- Result: `6 passed`

### Integration (Kafka/Compose)
- Command: `RUN_TRANSLATION_INTEGRATION=1 python -m pytest services/translation/tests/test_integration_translation.py -q`
- Result: `1 passed`
- Note: First run may take longer due to model download; integration timeout increased to 180s.

## How To Run (Local)
### Compose
- Start infra + services: `docker compose up --build`
- Services:
  - Kafka on `127.0.0.1:29092`
  - Schema Registry on `127.0.0.1:8081`
  - ASR service consumes `speech.audio.ingress` and publishes `speech.asr.text`
  - Translation service consumes `speech.asr.text` and publishes `speech.translation.text`

### Run integration test
- Prereqs: Compose stack running
- Command example:
  - `RUN_TRANSLATION_INTEGRATION=1 python -m pytest -m integration -q`

## Outstanding Items
- Model inference test is opt-in (`RUN_TRANSLATION_MODEL_TEST=1`) due to download/runtime cost.
- The default model is language-pair specific (`en`→`es`). If truly multilingual translation is required, switch to a multilingual model (heavier) and update validation.
- Integration test requires Docker/Kafka running and is disabled by default.
- QA/UAT prompt templates in the user home directory were not readable due to workspace tool restrictions; this report focuses on repo-local acceptance criteria.

## Next Steps
- QA: Run integration test with Compose up and validate translated output event shape.
- UAT: Validate end-to-end demo flow (Audio -> ASR -> Translation) using correlation IDs.
